services:
  # this downloads the models once on to host machine and exits
  model-downloader:
    image: rmrhub/model-downloader:v0.1.1
    volumes:
      - ./${DATA_DIR}:/app/${DATA_DIR} 
    restart: always 
    env_file:
      - .env

  # this is the inference-server
  inference-server:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    env_file:
      - .env
    ipc: "host"
    runtime: nvidia
    depends_on:
      - model-downloader    
    command: ["--model", "/models/${LLM_MODEL_NAME}", "--dtype", "${DTYPE}"]

  # this is the embeddings server
  embeddings-server:
    hostname: embeddings-server
    image: rmrhub/sia-embeddings-server:v0.1.1
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env  # Use environment variables from .env file
    volumes:
      - ./${DATA_DIR}:/app/${DATA_DIR}  # Mount shared data directory
    depends_on:
      - model-downloader
    ports:
      - "8002:8002"  # Expose port for the embeddings server API   
    restart: always
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # this is the api server
  api-server:
    hostname: api-server # required for web-server to proxy_pass
    image: rmrhub/sia-api-server:v0.1.1
    environment:
      - PYTHONUNBUFFERED=1 # can be removed in production
    env_file:
      - .env
    ports:
      - "8080:8080"
    volumes:
      - ./${DATA_DIR}:/app/${DATA_DIR}:rw  # Shared data directory
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      - embeddings-server
    restart: always     
    command: >
      sh -c "chmod -R 777 /app/${DATA_DIR} && uvicorn main:app --host 0.0.0.0 --port 8080 --workers ${API_NO_WORKERS:-1}"

  # this is the proxy cum web-server for chat
  web-server:
    image: rmrhub/sia-web-server:v0.1.1
    ports:
      - "80:80"
      # Uncomment the following line to expose HTTPS port
      # - "443:443"
    depends_on:
      - api-server
    restart: always 
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro

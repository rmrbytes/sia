# ------------ variables commonly used
#
# host system subdirectory that will contain all the data
DATA_DIR=data
# DEBUG
DEBUG=false
# The application will automatically create subdirectories
# called AGENTS, STORE and MODELS. Ensure the chmod is 777

# ------------ variables used by API-SERVER
#
# https: set to true if using https
USE_HTTPS=false
# no of workers
API_NO_WORKERS=4
# x-api key
X_API_KEY=
# used for JWT Tokens
SECRET_KEY=
# Duration of cookie in hours
ACCESS_TOKEN_EXPIRY_IN_HOURS=24
# embeddings-server details
EMBEDDINGS_SERVER=embeddings-server #name of the service
EMBEDDINGS_SERVER_PORT=8002 # port used by the embeddings-server
# llm-server details
LLM_SERVER=llm-server #name of the service
LLM_SERVER_PORT=8000 # port used by the llm-server
# Chat Request Parameters
# Response Length
CHAT_RESPONSE_LENGTH_DEFAULT=M # Medium, Short, Long
# Below are the thresholds for above abbr
CHAT_RESPONSE_LENGTH_SHORT=50,   # Short response tokens
CHAT_RESPONSE_LENGTH_MEDIUM=150 # Medium response tokens
CHAT_RESPONSE_LENGTH_LONG=300 # Long response tokesn
# Temperature for randomness in responses
CHAT_TEMPERATURE=0.7  # 0.0 (deterministic) to 1.0 (more creative)
# Maximum number of tokens to generate in a response
CHAT_MAX_TOKENS=200  # Adjust based on response length preference
# Top-p sampling parameter (nucleus sampling)
CHAT_TOP_P=0.9  # A higher value will allow more randomness
# Frequency penalty (reduces likelihood of repetitive words)
CHAT_FREQUENCY_PENALTY=0.0  # Values from 0.0 to 2.0
# Presence penalty (increases likelihood of introducing new concepts)
CHAT_PRESENCE_PENALTY=0.0  # Values from 0.0 to 2.0

# ------------ variables used by EMBEDDINGS-SERVER
#
# no of workers
EMBEDDINGS_NO_WORKERS=1
# model name for Named Entities
#NER_MODEL_NAME=dbmdz/bert-large-cased-finetuned-conll03-english
# model name used for tokenization during chunking
#TOKENIZER_MODEL_NAME=google-bert/bert-base-uncased
# model name of sentence-transformers type
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2 
# sentence-transformers/all-mpnet-base-v2
# pytorch_model.bin is the usual filename but kept it in .env for flexibility
EMBEDDING_MODEL_FILENAME=pytorch_model.bin 
# Hugging Face token. Some models may require this
HUGGING_FACE_HUB_TOKEN=
# Documents related
FILE_TYPES=txt,pdf,docx,md
# Chunking related
CHUNK_SIZE=100
CHUNK_OVERLAP=20
MAX_INPUT_TOKENS=2048
MIN_CHUNK_SIZE=50
MAX_CHUNK_COUNT=0
# chunk strategy: options are word, sentence, passage, page
CHUNK_STRATEGY=sentence
# retrieval strategy: options are knn,mrr,threshold,hybrid
RETRIEVAL_STRATEGY=knn
# for threshold
THRESHOLD_SIMILARITY=0.8
# distance metric Options: 'cosine', 'ip': inner product, 'l2': square L2
DISTANCE_METRIC=cosine
# no of results of query
TOP_N=1

# api-server details
API_SERVER=api-server #name of the service
API_SERVER_PORT=8080 # port used by the api-server

# ------------ variables used by used by LLM-SERVER
#
# Model name to use (LLM_MODEL_NAME is required)
LLM_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
# Data type for the model weights. 
# Options: "auto", "float16", "float32" (default)
DTYPE=float16  # Use float16 for faster inference with lower memory usage (recommended if GPU supports it)

# Number of workers for inference. Default is 1.
LLM_NUM_WORKERS=4  # Adjust based on the available CPU cores and the number of parallel requests expected

# Fraction of GPU memory to use. Float between 0 and 1.
VLLM_GPU_MEMORY_FRACTION=0.9  # Use 90% of GPU memory. Adjust based on available memory.

# Number of transformer layers to keep on GPU (if using offloading to CPU).
# Keep this lower for models that are too large to fit fully in GPU memory.
VLLM_NUM_GPU_LAYERS=40  # Set based on GPU memory. Increase if you want to keep more layers in GPU memory.

# Maximum sequence length in tokens. Default depends on the model (e.g., 2048 for GPT models).
VLLM_MAX_SEQ_LENGTH=4096  # Set based on the model's maximum sequence length. Recommended to match the model's capabilities.

# Enable offloading layers to CPU if GPU memory is insufficient.
VLLM_OFFLOAD_CPU=true  # Useful when GPU memory is low and model layers need to be offloaded to CPU.

# since models are loaded locally an OPEN API key is not required
# however in case of using it with Open AI's own model it is required
OPENAPI_KEY=None

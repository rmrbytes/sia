# ------------ variables commonly used
#
# Hugging Face token.
HUGGING_FACE_HUB_TOKEN=Set-Your-HF-Token
# x-api key
X_API_KEY=Generate-API-Key
# DEBUG
DEBUG=false
# host system subdirectory that will contain all the data
DATA_DIR=data
# The application will automatically create subdirectories
# called AGENTS, STORE and MODELS. Ensure the chmod is 777
#
# https: set to true if using https
USE_HTTPS=false
#
# ------------ variables used by API-SERVER
#
# used for Auth Tokens
SECRET_KEY=Secret-Key
# no of workers on API server
API_NO_WORKERS=4
# Duration of cookie in hours
ACCESS_TOKEN_EXPIRY_IN_HOURS=24
# embeddings-server details (name of the service in the docker compose)
EMBEDDINGS_SERVER=embeddings-server 
# port used by the embeddings-server in the docker compose
EMBEDDINGS_SERVER_PORT=8002 
# inference-server details (name of the service in the docker compose)
INFERENCE_SERVER=inference-server 
# port used by the inference-server in the docker compose
LLM_SERVER_PORT=8000 
#
# ------------ variables used by EMBEDDINGS-SERVER
#
# no of workers on Embeddings server
EMBEDDINGS_NO_WORKERS=1
# model name of sentence-transformers type
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2 
# pytorch_model.bin is the usual filename not used but kept it in .env for flexibility
EMBEDDING_MODEL_FILENAME=pytorch_model.bin 
# splitting strategy: options are word, sentence, passage, page
SPLIT_BY=sentence
# split length: number of items of above strategy in each chunk
SPLIT_LENGTH=5
# split overlap: number of items of split_by that should overlap across chunks
SPLIT_OVERLAP=0
# split threshold: min number of items in each chunk. If not available they are merged into previous chunk
SPLIT_THRESHOLD=0
# distance metric Options: 'cosine', 'ip': inner product, 'l2': square L2
DISTANCE_METRIC=cosine
# no of chunks retrieved from embeddings query
TOP_N=3
# api-server details (name of the service in the docker compose)
API_SERVER=api-server 
# port used by the embeddings-server in the docker compose
API_SERVER_PORT=8080 
#
# ------------ variables used by used by INFERENCE-SERVER
#
# Model name to use (LLM_MODEL_NAME is required)
LLM_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
# Data type for the model weights. 
# Options: "auto", "float16", "float32" (default)
# Use float16 for faster inference with lower memory usage (recommended if GPU supports it)
DTYPE=float16 
# Fraction of GPU memory to use. Float between 0 and 1. Adjust based on available memory.
VLLM_GPU_MEMORY_FRACTION=0.9 
# Number of transformer layers to keep on GPU (if using offloading to CPU).
# Keep this lower for models that are too large to fit fully in GPU memory.
VLLM_NUM_GPU_LAYERS=40  # Set based on GPU memory. Increase if you want to keep more layers in GPU memory.
# Maximum sequence length in tokens. Default depends on the model (e.g., 2048 for GPT models).
VLLM_MAX_SEQ_LENGTH=4096  # Set based on the model's maximum sequence length. Recommended to match the model's capabilities.
# Enable offloading layers to CPU if GPU memory is insufficient.
VLLM_OFFLOAD_CPU=true  # Useful when GPU memory is low and model layers need to be offloaded to CPU.
# since models are loaded locally an OPEN API key is not required
# however in case of using it with Open AI's own model it is required
OPENAPI_KEY=None
#
# ------------ variables used by CHAT CLIENT
#
# Response Length Buckets. Kept to enable user-settings
CHAT_RESPONSE_LENGTH_DEFAULT=M # Medium, Short, Long
# Below are the thresholds for above abbr
CHAT_RESPONSE_LENGTH_SHORT=50,   # Short response tokens
CHAT_RESPONSE_LENGTH_MEDIUM=150 # Medium response tokens
CHAT_RESPONSE_LENGTH_LONG=300 # Long response tokesn
# Temperature for randomness in responses
CHAT_TEMPERATURE=0.7  # 0.0 (deterministic) to 1.0 (more creative)
# Maximum number of tokens to generate in a response
CHAT_MAX_TOKENS=200  # Adjust based on response length preference
# Top-p sampling parameter (nucleus sampling)
CHAT_TOP_P=0.9  # A higher value will allow more randomness
# Frequency penalty (reduces likelihood of repetitive words)
CHAT_FREQUENCY_PENALTY=0.0  # Values from 0.0 to 2.0
# Presence penalty (increases likelihood of introducing new concepts)
CHAT_PRESENCE_PENALTY=0.0  # Values from 0.0 to 2.0
